import base64
import io
import logging
import time
import asyncio
from typing import List, Dict, Any, Optional

import cv2
import numpy as np
import mediapipe as mp # For facial landmarks
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import joblib # For loading the custom trained model, scaler, and encoder

# --- MediaPipe FaceMesh Initialization ---
# Using FaceMesh to get a rich set of landmarks (478 3D landmarks if refine_landmarks=True)
mp_face_mesh = mp.solutions.face_mesh
face_mesh_detector = mp_face_mesh.FaceMesh(
    max_num_faces=1,            # Process one face for this application
    refine_landmarks=True,      # Get detailed landmarks including iris, lips, etc. (478 landmarks)
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5 # Lower for smoother tracking in video, adjust as needed
)

# --- Custom Trained Model Loading ---
# These files should be generated by your 'conceptual_model_training.py' script
MODEL_PATH = "emotion_custom_model.joblib"  # e.g., your trained MLP, SVM, or RF
SCALER_PATH = "emotion_custom_scaler.joblib" # The StandardScaler fit on your training landmark data
ENCODER_PATH = "emotion_custom_encoder.joblib" # The LabelEncoder for emotion labels

emotion_classifier_model: Optional[Any] = None
landmark_data_scaler: Optional[Any] = None
emotion_label_mapping_encoder: Optional[Any] = None
default_emotion_list: List[str] = ["neutral", "happy", "sad", "angry", "surprise"] # Fallback if encoder fails

logging.basicConfig(level=logging.INFO) # Ensure logging is configured early
logger = logging.getLogger(__name__)

try:
    emotion_classifier_model = joblib.load(MODEL_PATH)
    landmark_data_scaler = joblib.load(SCALER_PATH)
    emotion_label_mapping_encoder = joblib.load(ENCODER_PATH)
    default_emotion_list = list(emotion_label_mapping_encoder.classes_)
    logger.info(f"Custom emotion classifier, scaler, and encoder loaded successfully.")
    logger.info(f"Available emotions from encoder: {default_emotion_list}")
except FileNotFoundError:
    logger.warning(f"Warning: One or more custom model files not found ({MODEL_PATH}, {SCALER_PATH}, {ENCODER_PATH}).")
    logger.warning("The application will use PLACEHOLDER emotion prediction logic.")
    logger.warning("Please train your model using the conceptual scripts and place the .joblib files in the backend directory.")
except Exception as e:
    logger.error(f"Error loading custom model/scaler/encoder: {e}. Using PLACEHOLDER logic.")


# Initialize FastAPI app
app = FastAPI(
    title="Facial Emotion Detection API (Custom Landmark-Based Model)",
    description="Streams video frames, detects facial landmarks using MediaPipe, and uses a custom-trained model for emotion classification.",
    version="2.1.0",
)

# CORS Configuration
origins = ["http://localhost:3000", "http://127.0.0.1:3000","https://facial-emotions2-0.vercel.app"] # Add your frontend URL if different
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models for WebSocket Communication ---
class LandmarkPoint(BaseModel):
    x: float
    y: float
    z: Optional[float] = None # z-coordinate from MediaPipe (relative depth)

class FrameAnalysisResponse(BaseModel):
    landmarks: Optional[List[LandmarkPoint]] = None # List of all detected landmark points
    dominant_emotion: Optional[str] = None
    emotion_probabilities: Optional[Dict[str, float]] = None # Probabilities for each emotion class
    processing_time_ms: float
    message: str
    error: bool = False
    error_message: Optional[str] = None

# --- Helper Functions ---
def decode_base64_image(base64_string: str) -> Optional[np.ndarray]:
    """Decodes a base64 image string to an OpenCV image (NumPy array)."""
    try:
        if "," in base64_string: # Handle data URL format (e.g., "data:image/jpeg;base64,...")
            _, encoded_data = base64_string.split(",", 1)
        else:
            encoded_data = base64_string # Assume raw base64 data
        
        decoded_bytes = base64.b64decode(encoded_data)
        image_array = np.frombuffer(decoded_bytes, dtype=np.uint8)
        cv2_image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        return cv2_image
    except Exception as e:
        logger.error(f"Error decoding base64 image: {e}")
        return None

def normalize_landmarks_for_prediction(image_width: int, image_height: int, landmarks_mp: List[Any]) -> Optional[np.ndarray]:
    """
    Normalizes MediaPipe landmarks for prediction.
    THIS FUNCTION MUST EXACTLY MATCH THE NORMALIZATION USED DURING TRAINING DATA PREPARATION.
    The version from 'conceptual_data_preparation.py' is replicated here.
    """
    if not landmarks_mp:
        return None

    pixel_landmarks = np.array([[lm.x * image_width, lm.y * image_height] for lm in landmarks_mp])
    centroid = np.mean(pixel_landmarks, axis=0)
    translated_landmarks = pixel_landmarks - centroid
    rms_distance = np.sqrt(np.mean(np.sum(translated_landmarks**2, axis=1)))
    
    if rms_distance == 0: return None

    normalized_landmarks_np = translated_landmarks / rms_distance
    flat_normalized_features = normalized_landmarks_np.flatten() # Features are x0,y0,x1,y1...

    # Ensure the number of features matches what the model was trained on (478 landmarks * 2 coords = 956 features)
    # This number depends on refine_landmarks=True for FaceMesh
    expected_feature_count = 478 * 2 
    if len(flat_normalized_features) != expected_feature_count:
        logger.warning(f"Normalized feature count mismatch. Expected {expected_feature_count}, got {len(flat_normalized_features)}. Check landmark count and normalization.")
        return None
    
    return flat_normalized_features.reshape(1, -1) # Reshape for scikit-learn model prediction (1 sample, N features)

# --- WebSocket Endpoint for Real-Time Emotion Detection ---
@app.websocket("/ws/emotion_detection") # New endpoint name
async def websocket_custom_emotion_detection(websocket: WebSocket):
    await websocket.accept()
    logger.info("WebSocket connection established for custom landmark-based emotion detection.")
    
    processing_times = [] # For averaging

    try:
        while True:
            base64_image_data = await websocket.receive_text()
            request_start_time = time.time()

            if not base64_image_data:
                continue

            cv2_frame = decode_base64_image(base64_image_data)
            if cv2_frame is None:
                await websocket.send_json(FrameAnalysisResponse(
                    processing_time_ms=(time.time() - request_start_time) * 1000,
                    message="Image decoding failed on server.", error=True
                ).dict())
                continue

            # Convert BGR image to RGB for MediaPipe
            rgb_frame = cv2.cvtColor(cv2_frame, cv2.COLOR_BGR2RGB)
            rgb_frame.flags.writeable = False # Optimize by making array non-writeable
            
            # Process the frame with MediaPipe FaceMesh
            results = face_mesh_detector.process(rgb_frame)
            rgb_frame.flags.writeable = True # Make it writeable again if needed later

            frame_height, frame_width, _ = rgb_frame.shape
            client_landmarks: Optional[List[LandmarkPoint]] = None
            predicted_emotion: Optional[str] = None
            emotion_probabilities_payload: Optional[Dict[str, float]] = None
            response_message = "Processing frame."

            if results.multi_face_landmarks:
                # Assuming one face, as per FaceMesh configuration
                face_landmarks_from_mp = results.multi_face_landmarks[0].landmark
                
                # Prepare raw landmark data (0-1 range) for the client to draw the mask
                client_landmarks = [
                    LandmarkPoint(x=lm.x, y=lm.y, z=lm.z) for lm in face_landmarks_from_mp
                ]

                # --- Custom Emotion Prediction using Trained Model ---
                if emotion_classifier_model and landmark_data_scaler and emotion_label_mapping_encoder:
                    # Normalize landmarks exactly as done during training
                    normalized_feature_vector = normalize_landmarks_for_prediction(frame_width, frame_height, face_landmarks_from_mp)
                    
                    if normalized_feature_vector is not None:
                        try:
                            # Scale the normalized features
                            scaled_feature_vector = landmark_data_scaler.transform(normalized_feature_vector)
                            
                            # Predict emotion probabilities
                            probabilities = emotion_classifier_model.predict_proba(scaled_feature_vector)[0]
                            
                            # Get the dominant emotion
                            predicted_label_index = np.argmax(probabilities)
                            predicted_emotion = emotion_label_mapping_encoder.inverse_transform([predicted_label_index])[0]
                            
                            # Create probabilities dictionary for client
                            emotion_probabilities_payload = {
                                emotion_label_mapping_encoder.inverse_transform([i])[0]: float(prob)
                                for i, prob in enumerate(probabilities)
                            }
                            response_message = "Emotion classified using custom model."
                        except Exception as model_pred_error:
                            logger.error(f"Error during custom model prediction: {model_pred_error}")
                            predicted_emotion = "prediction_error"
                            response_message = "Error occurred during model prediction."
                    else:
                        predicted_emotion = "normalization_failed"
                        response_message = "Landmark normalization failed; cannot predict emotion."
                else:
                    # PLACEHOLDER: Cycle through emotions if custom model isn't loaded
                    current_time_sec = int(time.time())
                    placeholder_emotion_index = current_time_sec % len(default_emotion_list)
                    predicted_emotion = default_emotion_list[placeholder_emotion_index]
                    emotion_probabilities_payload = {emo: (1.0 if emo == predicted_emotion else 0.0) for emo in default_emotion_list}
                    response_message = "Custom model not loaded; using placeholder emotion."
                
            else: # No face landmarks detected
                response_message = "No face detected in the frame."

            current_processing_time_ms = (time.time() - request_start_time) * 1000
            processing_times.append(current_processing_time_ms)
            if len(processing_times) > 30: # Keep last 30 timings
                processing_times.pop(0)
            # logger.info(f"Frame processed in {current_processing_time_ms:.2f} ms. Avg: {np.mean(processing_times):.2f} ms. Emotion: {predicted_emotion}")


            await websocket.send_json(FrameAnalysisResponse(
                landmarks=client_landmarks,
                dominant_emotion=predicted_emotion,
                emotion_probabilities=emotion_probabilities_payload,
                processing_time_ms=current_processing_time_ms,
                message=response_message
            ).dict())

    except WebSocketDisconnect:
        logger.info("WebSocket client disconnected.")
    except Exception as e:
        logger.error(f"Unexpected error in WebSocket: {e}", exc_info=True)
        try:
            await websocket.send_json(FrameAnalysisResponse(
                processing_time_ms=0, message="Critical server error.", error=True, error_message=str(e)
            ).dict())
        except Exception: # If sending fails, connection is likely already gone
            logger.error("Failed to send error message to disconnected WebSocket client.")
    finally:
        logger.info("Closing WebSocket connection handler.")
        # face_mesh_detector.close() # This might be too aggressive if shared, manage lifespan instead

@app.on_event("startup")
async def startup_event():
    logger.info("Application startup: Initializing resources...")
    # MediaPipe FaceMesh is already initialized globally.
    # Models are attempted to be loaded globally.
    # Any other one-time setup can go here.

@app.on_event("shutdown")
def shutdown_event():
    logger.info("Application shutdown: Cleaning up resources...")
    face_mesh_detector.close() # Ensure MediaPipe resources are released
    logger.info("MediaPipe FaceMesh resources closed.")

# Root endpoint for API info
@app.get("/")
async def get_api_info():
    model_status = "Loaded" if emotion_classifier_model and landmark_data_scaler and emotion_label_mapping_encoder else "Not Loaded (Using Placeholder)"
    return {
        "api_name": "Custom Facial Emotion Detection API",
        "version": app.version,
        "description": "Detects facial landmarks and uses a custom model for emotion classification.",
        "websocket_endpoint": "/ws/emotion_detection",
        "custom_model_status": model_status,
        "expected_emotions": default_emotion_list
    }

# To run the server (if running this file directly):
# for running this script please run this command into terminal in root directory:
# uvicorn main:app --reload --host 0.0.0.0 --port 8000
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)