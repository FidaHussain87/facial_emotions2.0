## Conceptual Python Script: Training Emotion Classifier from Landmarks

#This script demonstrates how to train a machine learning model (e.g., a Multi-Layer Perceptron - MLP) using the CSV file of normalized facial landmarks generated by the `conceptual_data_preparation.py` script.

#**You will need to:**
#1.  Install necessary libraries: `pandas`, `scikit-learn`, `joblib`.
#   ```bash
#   pip install pandas scikit-learn joblib
#    ```
#2.  Ensure the CSV file (e.g., `facial_landmarks_for_emotion_training.csv`) from the data preparation step exists in the same directory or provide the correct path.
# sequence would be ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'] -> [0 1 2 3 4 5 6]
#```python
# conceptual_model_training.py
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib # For saving the model, scaler, and encoder
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    """
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

def train_emotion_model(csv_path, 
                        model_type='mlp', # 'mlp', 'svm', 'rf'
                        model_output_path="emotion_custom_model.joblib", 
                        scaler_output_path="emotion_custom_scaler.joblib", 
                        encoder_output_path="emotion_custom_encoder.joblib"):
    """
    Trains an emotion classifier on the facial landmark data.

    Args:
        csv_path (str): Path to the CSV file containing landmark data.
        model_type (str): Type of model to train ('mlp', 'svm', 'rf').
        model_output_path (str): Path to save the trained model.
        scaler_output_path (str): Path to save the feature scaler.
        encoder_output_path (str): Path to save the label encoder.
    """
    print(f"--- Starting Emotion Model Training (Model Type: {model_type.upper()}) ---")
    
    # 1. Load the dataset
    try:
        dataset = pd.read_csv(csv_path)
        if dataset.empty:
            print(f"Error: The CSV file at {csv_path} is empty. Please generate data first.")
            return
        print(f"Dataset loaded successfully from {csv_path}. Shape: {dataset.shape}")
        print("Dataset head:\n", dataset.head())
    except FileNotFoundError:
        print(f"Error: CSV file not found at {csv_path}. Please run data preparation first.")
        return
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return

    # 2. Prepare data: Separate features (X) and target (y)
    try:
        X = dataset.drop('emotion', axis=1) # All columns except 'emotion' are features
        y_labels = dataset['emotion']      # 'emotion' column is the target
    except KeyError:
        print("Error: 'emotion' column not found in CSV. Ensure it's the first column and named correctly.")
        return
        
    if X.shape[1] == 0:
        print("Error: No feature columns found. Check CSV structure. Expected landmark columns like 'lm_0_x', 'lm_0_y', ...")
        return

    print(f"Number of features (landmark coordinates): {X.shape[1]}")
    print(f"Number of samples: {X.shape[0]}")
    print(f"Emotion distribution:\n{y_labels.value_counts()}")

    # 3. Encode emotion labels (string to numerical)
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y_labels)
    emotion_classes = list(label_encoder.classes_)
    print(f"Encoded emotion classes: {emotion_classes} -> {np.unique(y_encoded)}")

    # 4. Split data into training and testing sets
    # stratify=y_encoded ensures similar class distribution in train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded
    )
    print(f"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}")

    # 5. Scale numerical features (landmark coordinates)
    # StandardScaler standardizes features by removing the mean and scaling to unit variance.
    # Fit only on training data, then transform both training and test data.
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("Features scaled using StandardScaler.")

    # 6. Train the selected model
    print(f"\n--- Training {model_type.upper()} Model ---")
    
    model = None
    if model_type == 'mlp':
        # Multi-Layer Perceptron Classifier
        # Hyperparameters here are examples; use GridSearchCV for tuning
        model = MLPClassifier(
            hidden_layer_sizes=(256, 128, 64), 
            activation='relu',
            solver='adam',
            alpha=0.001, # L2 regularization
            learning_rate_init=0.001,
            max_iter=700, # Increase if convergence issues
            random_state=42,
            early_stopping=True,
            n_iter_no_change=30, # Stop if no improvement
            verbose=False # Set to True for more training logs
        )
        # Example for GridSearchCV (can be time-consuming)
        # param_grid = {
        #     'hidden_layer_sizes': [(128,), (128, 64), (256, 128, 64)],
        #     'alpha': [0.0001, 0.001, 0.01],
        #     'learning_rate_init': [0.001, 0.0005]
        # }
        # grid_search = GridSearchCV(MLPClassifier(max_iter=500, random_state=42, early_stopping=True, solver='adam'), param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)
        # grid_search.fit(X_train_scaled, y_train)
        # print(f"Best MLP parameters: {grid_search.best_params_}")
        # model = grid_search.best_estimator_

    elif model_type == 'svm':
        # Support Vector Machine
        # For SVM, parameter tuning (C, kernel, gamma) is crucial.
        model = SVC(probability=True, random_state=42, C=1.0, kernel='rbf', gamma='scale') 
        # Example for GridSearchCV
        # param_grid_svm = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.01], 'kernel': ['rbf', 'poly']}
        # grid_search_svm = GridSearchCV(SVC(probability=True, random_state=42), param_grid_svm, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)
        # grid_search_svm.fit(X_train_scaled, y_train)
        # print(f"Best SVM parameters: {grid_search_svm.best_params_}")
        # model = grid_search_svm.best_estimator_
        
    elif model_type == 'rf':
        # Random Forest Classifier
        model = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, min_samples_split=5, min_samples_leaf=2)
        # Example for GridSearchCV
        # param_grid_rf = {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5, 10]}
        # grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)
        # grid_search_rf.fit(X_train_scaled, y_train)
        # print(f"Best RF parameters: {grid_search_rf.best_params_}")
        # model = grid_search_rf.best_estimator_
    else:
        print(f"Error: Unknown model type '{model_type}'. Choose from 'mlp', 'svm', 'rf'.")
        return

    if model:
        model.fit(X_train_scaled, y_train)
        print(f"{model_type.upper()} model training complete.")

        # 7. Evaluate the model
        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)

        train_accuracy = accuracy_score(y_train, y_pred_train)
        test_accuracy = accuracy_score(y_test, y_pred_test)

        print(f"\n--- Model Evaluation ({model_type.upper()}) ---")
        print(f"Training Accuracy: {train_accuracy:.4f}")
        print(f"Test Accuracy: {test_accuracy:.4f}")

        print("\nClassification Report (Test Set):")
        print(classification_report(y_test, y_pred_test, target_names=emotion_classes, zero_division=0))
        
        print("\nPlotting Confusion Matrix for Test Set...")
        plot_confusion_matrix(y_test, y_pred_test, classes=emotion_classes)

        # 8. Save the model, scaler, and label encoder
        joblib.dump(model, model_output_path)
        joblib.dump(scaler, scaler_output_path)
        joblib.dump(label_encoder, encoder_output_path)
        print(f"\nTrained {model_type.upper()} model saved to: {model_output_path}")
        print(f"Scaler saved to: {scaler_output_path}")
        print(f"Label encoder saved to: {encoder_output_path}")
        print("\n--- Training Process Finished ---")
    else:
        print("Model was not initialized.")


# --- Main Execution ---
if __name__ == '__main__':
    # Path to the CSV file generated by conceptual_data_preparation.py
    landmark_data_csv = "facial_landmarks_for_emotion_training.csv" 

    # Choose model type: 'mlp', 'svm', or 'rf'
    # MLP is often a good starting point for this kind of feature set.
    # SVMs can also work well. Random Forests are robust.
    # Hyperparameter tuning (e.g., with GridSearchCV) is ESSENTIAL for good performance.
    
    # Check if CSV exists before proceeding
    if not os.path.exists(landmark_data_csv):
        print(f"Error: Landmark data CSV file '{landmark_data_csv}' not found.")
        print("Please run the 'conceptual_data_preparation.py' script first to generate this file.")
    else:
        # You can try different models by changing the model_type parameter
        train_emotion_model(landmark_data_csv, model_type='mlp')
        # train_emotion_model(landmark_data_csv, model_type='svm')
        # train_emotion_model(landmark_data_csv, model_type='rf')
